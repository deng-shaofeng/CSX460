---
title: "Classification Metrics"
author: "Shaofeng Deng"
date:  "November 8, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(MASS)
library(readr)
library(lubridate)
library(magrittr)
library(lubridate)
library(stringr)
library(dplyr)
library(data.table)
```

The German Credit Data (data/german_credit); 

1. Read in the German Credit Data
2. Partition the model into Test and Training Sets using only `base::sample` 
3. Train a model for `Creditability` 
4. Knit the document and submit both the this file Rmd and html files.

Show All Work! 

```{r}
# Read data
credit <- read.csv("../data/german_credit.csv")

# Partition into test and training sets
smp_size <- floor(0.7 * nrow(credit))
train_num <- sample(seq_len(nrow(credit)), size = smp_size)
train <- credit[train_num, ]
test <- credit[-train_num, ]

# Train the model
mymodel <- glm(Creditability ~ ., data = train, family = "binomial")
summary(mymodel)
```


Using the `predict` function and `test` data, write functions to calculate and 
calculate: 

* Misclassification Rate
* Prevalence 
* Accuracy
* Accuracy
* Error Rate / Misclassification Rate
* True Positive Rate  
* False Positive Rate
* True Negative Rate  
* False Negative Rate 
* Sensitivity 
* Specificity 
* Recall 
* Precision

```{r, echo=FALSE}
result <- predict(mymodel, test, type = "response")
result[result > 0.5] <- 1
result[result <= 0.5] <- 0
TP <- sum(result == test$Creditability & result == 1)
TN <- sum(result == test$Creditability & result == 0)
FP <- sum(result != test$Creditability & result == 1)
FN <- sum(result != test$Creditability & result == 0)
n <- TP+TN+FP+FN

Prevalence <- (TP+FP)/n
Accuracy <- (TP+TN)/n
Error_Rate <- (FP+FN)/n
TPR <- TP/(TP+FP)
FPR <- FP/(TP+FP)
TNR <- TN/(TN+FN)
FNR <- FN/(TN+FN)
Sensitivity <- TPR
Specificity <- TNR
Recall <- TPR
Precision <- TP/sum(result == 1)


```
